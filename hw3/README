1.File list
    train:
        run_train.sh
            launch the model trainning process on spark
        gen_sentiment_dic.py
            generate word sentiment dictionary
        yahoo_quotes.py
            get stock return info from spark
        step1_get_tweets_info.py
            extract useful tweets json file into textfile
        step2_join_stock_ret_with_tweets.py
            combine stock ret with tweets by date and stockid
        step3_extract_feature.py
            extract feature from the tweets content
        step4_train_model.py
            training a linear model

    prediction:
        run_prediction.sh
            launch the prediction process on spark
        step1_gen_pred_input.py
            generate predction data from tweets info
        step2_prediction.py
            load the trained model and make prediction on the input tweet info

    data:
        small sample of data from hdfs
        train:
            step1:
                file format:    date \t stockid \t  [tweet_info]
            step2:
                file format:    date \t stockid \t  stock_ret \t [tweet_info]
            step3:
                file format:    ret \t [0,0.....fea_i_j,....0,0]
            step4:
                file format:    model_param1, model_param2, ....model_paramN
        pred:
            step1:
                file format:    stockid \t [0,0.....fea_i_j,....0,0]
            step2:
                file format:    stockid \t predicted_ret
                
2. how to run
    There are trainning parts and prediction parts in this package
    bash run_train.sh  starts trainning on spark
    bash run_prediction.sh starts prediction on spark

3. the detailed process
    train:
        Although all the process can merged into one program for efficiency, 
        I split it into several step, so that I can check if every step is correct,
        and I can have the in-process product.

        step1:
            extract useful tweets json file into textfile, it judges if a file is related 
            to a certain stock
        step2:
            It combine stock ret with tweets by date and stockid, 
            so that the tweets info are associated with its related stocks later return
        step3:
            It extracts feature from the tweets content, I only use the ratio of good words
            and bad words as feature, although many other feature can be used. 
            If the feature size is M, and there are N stock, the feature vector length is N*M
            For example if there are 5 stock, every stock has 2 feature, an instance is like:
            0.0015 \t 0, 0, 0.032, 0.001, 0, 0, 0, 0
            0.0015 is return, this stockid is 1, so 0.032, 0.001 locates at 2,3 in the vector
        step4:
            It trains a linear model on the instances from last step

    prediction:            
        step1:
            Extract tweets json file into textfile and extract feature from it 
        step2:
            Load the model from trainning and make prediction on the prediction data

            
            
